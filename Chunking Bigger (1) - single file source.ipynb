{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "After [setting up](#Set-up) the python environment, we are going to:\n",
    "- read the source file into memory\n",
    "- determine the variable on which we will split files\n",
    "- create an ordered list of values for that variable and save it as an object\n",
    "- recursively do the following until we've done this for all chunks\n",
    "    - overwrite the df with the slice that represents a chunk\n",
    "    - add headers to the file\n",
    "    - create a name for the file and save as an object\n",
    "    - write the resulting file out to a folder with that name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io, glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Reading of Data (1st Lambda)\n",
    "\n",
    "[back to top](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will flow through three folders:\n",
    "- the input folder: \n",
    "    this folder holds the large files which need to be chunked\n",
    "- the intermediate folder: \n",
    "    this folder will hold intermediate files which have data associated with an individual booklet.  The data in these files may not contain all events associated with a particular student, depending on the value assigned to the chunksize argument supplied to pandas.read_csv and the size of the data associate with that student \n",
    "- output folder:\n",
    "    This is the folder where the final chunks will be placed. Each file here contains all of the events associated with the student indicated in the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFolder = 'C:/Users/cagard/OneDrive - Educational Testing Service/Testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# This function \n",
    "# x = The assigned student number\n",
    "# chunks = the number of individual chunks to assign\n",
    "####################################################\n",
    "def assignChunk(x,chunks=chunks):\n",
    "    return min([c for c in chunks if x<c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates the reference dataframe that deals with a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 358228 unique Booklets in the source file.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/cagard/Downloads/ST4/2017Math_1717MA4D01GXXX02EX_obs_tab_delimited_headers_CA.csv')\n",
    "\n",
    "refdf = pd.DataFrame(df.BookletNumber.unique())\n",
    "refdf = refdf.reset_index().rename(columns={'index':'Student', 0:'BookletNumber'})\n",
    "print(\"There are %d unique Booklets in the source file.\"%refdf.BookletNumber.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Data (also 1st Lambda)\n",
    "Here we will try to save chunks. For each chunk in the file, we will:\n",
    "- identify the list of booklets in that chunk and save all data for each booklet to it's own file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks 359\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "#set number of booklets to a chunk  This is the same for either type of file\n",
    "####################################################\n",
    "chunksize = 1000\n",
    "nchunks = int(np.ceil(refdf['Student'].max()/1000))\n",
    "print('number of chunks',nchunks)\n",
    "\n",
    "chunks = [(c+1) *chunksize for c in range(nchunks)]\n",
    "chunks\n",
    "####################################################\n",
    "# take the 'Student' column and pass it to the function\n",
    "# assignChunk() and assign the response to a new\n",
    "# column in refdf.\n",
    "####################################################\n",
    "refdf['chunk'] = refdf['Student'].apply(assignChunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Code improvement: creation of BlockCode column works for the current format\n",
    "# must be improved for other formats (standard file format). \n",
    "# Ideally we should do this with regular expression\n",
    "####################################################\n",
    "\n",
    "# refdf['BlockCode'] = refdf.File[0].split('_')[-3].split('.csv')[0]\n",
    "refdf['BlockCode'] = df.BlockCode[0]\n",
    "for chunk in refdf.chunk.unique():\n",
    "    try:\n",
    "        refdf['startEnd'] = refdf.chunk.apply(lambda x: str(x-chunksize+1)+'-'+str(x))\n",
    "#         print('refdf', refdf['startEnd'])\n",
    "    #     refdf.startEnd.unique()\n",
    "        #merge to file\n",
    "#         dfLarge = dfLarge.merge(refdf,on='BookletNumber')\n",
    "        refdf['fileType'] = \"raw\"\n",
    "        refdf['outfile'] = refdf.fileType + \"_\" + refdf.BlockCode.astype(str)+'_'+refdf.startEnd.astype(str)+'.csv'\n",
    "        refdf.head()\n",
    "    except:\n",
    "        print('exception')\n",
    "        #write files to chunks\n",
    "print('done')\n",
    "\n",
    "# Single file\n",
    "# print(df.shape)\n",
    "\n",
    "df = df.merge(refdf, how=\"left\", on=['BookletNumber', 'BlockCode'])\n",
    "# print(df.shape)\n",
    "# df.head()\n",
    "\n",
    "# single file\n",
    "df.groupby('outfile').apply(lambda x: x.to_csv(outputFolder+x.outfile[0], index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking outfiles\n",
    "We will read in a few of the files that were just generated from the code above. \n",
    "Checking the distribution of students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n"
     ]
    }
   ],
   "source": [
    "outFileList = glob.glob(outputFolder+\"raw_1717*.csv\")\n",
    "print(len(outFileList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################\n",
    "# Note: No more than one file can have fewer number of Students than the chunk size\n",
    "#     No files should have more than the number of chunks. \n",
    "#     All files should have the same number of columns.\n",
    "# This will show how many students as well as the shape (columns)\n",
    "####################################################\n",
    "# df = pd.read_csv(outFileList[0])\n",
    "# print(df.BookletNumber.nunique(), df.shape[1])\n",
    "# df.head()\n",
    "colList = []\n",
    "# bookNumList\n",
    "for file in outFileList:\n",
    "    tempCols = pd.read_csv(file,nrows=5).columns\n",
    "    colList.append(len(tempCols))\n",
    "#     tempBookletNumbers = pd.read_csv(file,usecols = ['BookletNumber']).nunique()\n",
    "    \n",
    "#     print('File {}: Number of Students (Booklet Numbers) {}\\n Number of columns {} (columns names: {})\\n'\\\n",
    "#           .format(file, tempBookletNumbers, len(tempCols), tempCols))\n",
    "list(set(colList))\n",
    "# uniqueColList = [subList for subList in colList if len([sL for sL in colList if outFileList>1])!>1]\n",
    "# if len(uniqueColList)>0:\n",
    "#     print(\"The following sets of columns are unique among the processed files\")\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
